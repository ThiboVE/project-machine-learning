{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14573d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a224a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device,\n",
    "                 latent_dim, gru_dim, vocab_size, embedding_dim,\n",
    "                 teacher_forcing_ratio=0.5):\n",
    "        super(cVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gru_dim = gru_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "        # Token embedding (for decoder)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Latent space projections\n",
    "        self.encoder_mu = nn.Linear(gru_dim, latent_dim)\n",
    "        self.encoder_logvar = nn.Linear(gru_dim, latent_dim)\n",
    "\n",
    "    def _sample_latent(self, hidden_encoder):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z ~ N(mu, sigma^2)\n",
    "        \"\"\"\n",
    "        mu = self.encoder_mu(hidden_encoder)\n",
    "        logvar = self.encoder_logvar(hidden_encoder)\n",
    "        sigma = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(sigma).to(self.device)\n",
    "        z = mu + sigma * eps\n",
    "\n",
    "        # Save for loss calculation\n",
    "        self.z_mean = mu\n",
    "        self.z_logvar = logvar\n",
    "\n",
    "        return z\n",
    "\n",
    "    def forward_decoder(self, z, x, y):\n",
    "        \"\"\"\n",
    "        Autoregressive decoding\n",
    "        z: [batch, latent_dim]\n",
    "        x: [batch, seq_len] token indices\n",
    "        y: [batch, 1] property vector\n",
    "        \"\"\"\n",
    "        batch_size, target_len = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        outputs = torch.zeros(batch_size, target_len, self.vocab_size).to(device)\n",
    "\n",
    "        # Initialize first token as <STR> (index 2)\n",
    "        input_token = torch.ones(batch_size, dtype=torch.long).to(device) * 2\n",
    "        outputs[:,0,2] = 1\n",
    "\n",
    "        # Initialize hidden state (zeros)\n",
    "        hidden = torch.zeros(self.decoder.n_layers, batch_size, self.decoder.gru_size).to(device)\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden = self.decoder(input_token, z, y, hidden)\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            # Get predicted token\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            # Teacher forcing\n",
    "            if random.random() < self.teacher_forcing_ratio:\n",
    "                input_token = x[:, t]  # ground truth\n",
    "            else:\n",
    "                input_token = top1.detach()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Full forward pass\n",
    "        x: [batch, seq_len] token indices\n",
    "        y: [batch, 1] property vector\n",
    "        \"\"\"\n",
    "        # Encode graph to hidden representation\n",
    "        hidden_encoder = self.encoder(x)  # should output [batch, gru_dim]\n",
    "\n",
    "        # Sample latent vector\n",
    "        z = self._sample_latent(hidden_encoder)\n",
    "\n",
    "        # Decode sequence\n",
    "        recon_x = self.forward_decoder(z, x, y)\n",
    "\n",
    "        return recon_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0bbbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, latent_dim, gru_size, n_layers, embedding_dim):\n",
    "        super(GRU_Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gru_size = gru_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Embedding layer for input tokens\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # GRU input size: token embedding + latent vector + property\n",
    "        self.gru = nn.GRU(embedding_dim + latent_dim + 1, gru_size, n_layers, batch_first=True)\n",
    "\n",
    "        # Output layer: project GRU hidden state to vocab size\n",
    "        self.fc = nn.Linear(gru_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_token, z, y, hidden):\n",
    "        \"\"\"\n",
    "        input_token: [batch] token indices\n",
    "        z: [batch, latent_dim]\n",
    "        y: [batch, 1] property vector\n",
    "        hidden: [n_layers, batch, gru_size]\n",
    "        \"\"\"\n",
    "        # Embed token\n",
    "        token_embed = self.embedding(input_token)  # [batch, embed_dim]\n",
    "\n",
    "        # Concatenate token embedding + latent + property\n",
    "        decoder_input = torch.cat([token_embed, z, y], dim=1).unsqueeze(1)  # [batch, 1, embed+latent+1]\n",
    "\n",
    "        # GRU forward\n",
    "        output, hidden = self.gru(decoder_input, hidden)  # output: [batch, 1, gru_size]\n",
    "\n",
    "        # Project to vocab\n",
    "        output = self.fc(output.squeeze(1))  # [batch, vocab_size]\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453524d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load QM9 SMILES\n",
    "df_qm9 = pd.read_pickle('data/RDKit/rdkit_only_valid_smiles_qm9.pkl')\n",
    "smiles_list = df_qm9[\"SMILES\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827659d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', '(', ')', '+', '-', '/', '1', '2', '3', '4', '5', '=', '@', 'C', 'F', 'H', 'N', 'O', '[', '\\\\', ']']\n",
      "Vocabulary size: 24\n",
      "Example tokens: ['<PAD>', '<END>', '<STR>', '#', '(', ')', '+', '-', '/', '1', '2', '3', '4', '5', '=', '@', 'C', 'F', 'H', 'N', 'O', '[', '\\\\', ']']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Collect all unique characters\n",
    "charset = set()\n",
    "for smi in smiles_list:\n",
    "    for ch in smi:\n",
    "        charset.add(ch)\n",
    "\n",
    "# Sort for consistency\n",
    "charset = sorted(list(charset))\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = ['<PAD>', '<END>', '<STR>']\n",
    "vocab_list = special_tokens + charset\n",
    "\n",
    "# Create token -> index mapping\n",
    "token2idx = {tok: idx for idx, tok in enumerate(vocab_list)}\n",
    "idx2token = {idx: tok for tok, idx in token2idx.items()}\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab_list))\n",
    "print(\"Example tokens:\", vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3040cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "batch_size = 4\n",
    "seq_len = 15                     # sequence length for SMILES\n",
    "vocab_size = len(vocab_list)     # small example vocab\n",
    "latent_dim = 16\n",
    "gru_dim = 32\n",
    "embedding_dim = 8\n",
    "n_layers = 1\n",
    "\n",
    "device = 'cpu'      # or 'cuda' if GPU is available\n",
    "\n",
    "# Dummy input tokens (batch of sequences)\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
    "\n",
    "# Dummy target property (HOMO-LUMO gap, for example)\n",
    "y = torch.rand(batch_size, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f51be5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10,  1,  2, 18, 19,  0, 18,  6,  5,  2,  6,  1, 13, 23, 22],\n",
      "        [17, 17, 17, 13,  0, 13, 17,  4,  0, 17,  5,  3, 17, 22, 14],\n",
      "        [ 3,  9,  1, 17,  0, 21, 13, 16, 20, 21, 16,  2, 11,  0,  8],\n",
      "        [ 9,  5, 22, 14,  8,  9, 10,  8, 14,  1, 23, 21, 10, 19, 20]])\n",
      "tensor([[0.0325],\n",
      "        [0.5752],\n",
      "        [0.3310],\n",
      "        [0.5041]])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5b521d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEncoder(nn.Module):\n",
    "    def __init__(self, gru_dim):\n",
    "        super().__init__()\n",
    "        self.gru_dim = gru_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # just return random vector as hidden representation\n",
    "        return torch.rand(batch_size, self.gru_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3482933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = GRU_Decoder(\n",
    "    vocab_size=vocab_size,\n",
    "    latent_dim=latent_dim,\n",
    "    gru_size=gru_dim,\n",
    "    n_layers=n_layers,\n",
    "    embedding_dim=embedding_dim\n",
    ").to(device)\n",
    "\n",
    "encoder = DummyEncoder(gru_dim=gru_dim).to(device)\n",
    "\n",
    "model = cVAE(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    device=device,\n",
    "    latent_dim=latent_dim,\n",
    "    gru_dim=gru_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=0.5\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8dcb889c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 15, 24])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(x, y)  # [batch, seq_len, vocab_size]\n",
    "    \n",
    "print(\"Output shape:\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "767c2f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token indices:\n",
      " tensor([[ 2,  3,  3,  3,  3,  3, 13,  3,  3, 13,  3,  3,  3,  3, 13],\n",
      "        [ 2,  9,  9,  9,  9, 11, 11,  9,  9, 11,  9,  9,  9,  9, 11],\n",
      "        [ 2,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
      "        [ 2, 12,  3,  6,  6,  3, 18,  6,  3, 18,  3,  3,  3, 18,  3]])\n"
     ]
    }
   ],
   "source": [
    "# pick the most probable token at each step\n",
    "pred_tokens = outputs.argmax(-1)\n",
    "print(\"Predicted token indices:\\n\", pred_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82b86836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4#++#H+#H###H#'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([idx2token[i] for i in pred_tokens[3].numpy()][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e88d762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "')\\\\=/12/=<END>][2NO'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([idx2token[i] for i in x[3].numpy()][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9564483e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
