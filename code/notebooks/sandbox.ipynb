{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14573d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from library.GCN import ConvolutionLayer, PoolingLayer, GraphData, collate_graph_dataset, Standardizer, Graph\n",
    "from library.cVAE import GCN_Encoder, GRU_Decoder, cVAE\n",
    "from torch.utils.data import DataLoader,SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab3f2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 24\n",
      "Example tokens: ['<PAD>', '<END>', '<STR>', '#', '(', ')', '+', '-', '/', '1', '2', '3', '4', '5', '=', '@', 'C', 'F', 'H', 'N', 'O', '[', '\\\\', ']']\n"
     ]
    }
   ],
   "source": [
    "# Load QM9 SMILES\n",
    "df_qm9 = pd.read_pickle('../data/RDKit/rdkit_only_valid_smiles_qm9.pkl')\n",
    "smiles_list = df_qm9[\"SMILES\"].to_list()\n",
    "\n",
    "# Collect all unique characters\n",
    "charset = set()\n",
    "for smi in smiles_list:\n",
    "    for ch in smi:\n",
    "        charset.add(ch)\n",
    "\n",
    "# Sort for consistency\n",
    "charset = sorted(list(charset))\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = ['<PAD>', '<END>', '<STR>']\n",
    "vocab_list = special_tokens + charset\n",
    "\n",
    "# Create token -> index mapping\n",
    "token2idx = {tok: idx for idx, tok in enumerate(vocab_list)}\n",
    "idx2token = {idx: tok for tok, idx in token2idx.items()}\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab_list))\n",
    "print(\"Example tokens:\", vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d48a525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    epoch,\n",
    "    model,\n",
    "    training_dataloader,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    use_GPU,\n",
    "    max_atoms,\n",
    "    node_vec_len,\n",
    "):\n",
    "    \"\"\"\n",
    "    Custom function which defines how a model will be trained (per epoch), here the mean-squared loss between prediction and actual value is used as evaluation metric. This function will perform backpropagation which updates the weights of the networks based in this evaluation.\n",
    "    \"\"\"\n",
    "    # Create variables to store losses and error\n",
    "    avg_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    # Switch model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # Go over each batch in the dataloader\n",
    "    for i, dataset in enumerate(training_dataloader):\n",
    "        # Unpack data\n",
    "        node_mat = dataset[0][0]\n",
    "        adj_mat = dataset[0][1]\n",
    "        gap = dataset[1]\n",
    "\n",
    "        smiles = []\n",
    "        for smile in dataset[2]:\n",
    "            char_list = ['<STR>'] + list(smile) + ['<END>']\n",
    "            vocab_idx_list = torch.as_tensor([token2idx[ch] for ch in char_list])\n",
    "            smiles.append(vocab_idx_list)\n",
    "\n",
    "        # ---- Pad sequences ----\n",
    "        # find max length\n",
    "        max_seq_len = max(s.size(0) for s in smiles)\n",
    "\n",
    "        # pad with a PAD token index\n",
    "        PAD_IDX = token2idx['<PAD>']\n",
    "\n",
    "        padded_smiles = torch.full((len(smiles), max_seq_len), PAD_IDX, dtype=torch.long)\n",
    "\n",
    "        for i, seq in enumerate(smiles):\n",
    "            padded_smiles[i, :seq.size(0)] = seq\n",
    "        \n",
    "        batch_size, _ = padded_smiles.size()\n",
    "\n",
    "        # Reshape inputs\n",
    "        node_mat = node_mat.reshape(batch_size, max_atoms, node_vec_len)\n",
    "        adj_mat = adj_mat.reshape(batch_size, max_atoms, max_atoms)\n",
    "\n",
    "        # Package inputs and outputs; check if GPU is enabled\n",
    "        if use_GPU:\n",
    "            model_input = (node_mat.cuda(), adj_mat.cuda(), padded_smiles, gap.cuda())\n",
    "            model_output = padded_smiles\n",
    "        else:\n",
    "            model_input = (node_mat, adj_mat, padded_smiles, gap)\n",
    "            model_output = padded_smiles\n",
    "\n",
    "        # Compute output from network\n",
    "        model_prediction_distribution = model(*model_input) # [batch_size, max_smiles_seq_len, vocab_size]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(model, model_prediction_distribution.permute(0, 2, 1), model_output, batch_size)\n",
    "        avg_loss += loss\n",
    "\n",
    "        # Calculate MAE\n",
    "        # prediction = standardizer.restore(nn_prediction.detach().cpu())\n",
    "        # mae = mean_absolute_error(output, prediction)\n",
    "        # avg_mae += mae\n",
    "\n",
    "        # Set zero gradients for all tensors\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Do backward prop\n",
    "        loss.backward()\n",
    "\n",
    "        # Update optimizer parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Increase count\n",
    "        count += 1\n",
    "\n",
    "    # Calculate avg loss and MAE\n",
    "    avg_loss = avg_loss / count\n",
    "    # avg_mae = avg_mae / count\n",
    "\n",
    "    # Print stats\n",
    "    # print(\n",
    "    #     \"Epoch: [{0}]\\tTraining Loss: [{1:.2f}]\\tTraining MAE: [{2:.2f}]\"\\\n",
    "    #        .format(\n",
    "    #                 epoch, avg_loss, avg_mae\n",
    "    #        )\n",
    "    # )\n",
    "\n",
    "    print(\n",
    "        \"Epoch: [{0}]\\tTraining Loss: [{1:.2f}]\".format(epoch, avg_loss)\n",
    "    )\n",
    "\n",
    "    # Return loss and MAE\n",
    "    return avg_loss #, avg_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0c9b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Inputs\n",
    "n_epochs = 15\n",
    "batch_size = 1000\n",
    "train_size = 0.7\n",
    "learning_rate = 0.01\n",
    "device = \"cpu\"\n",
    "\n",
    "# GCN\n",
    "max_atoms = 30 # fixed value\n",
    "node_vec_len = 16 # fixed value\n",
    "n_features = 32\n",
    "n_conv_layers = 2\n",
    "n_hidden_layers = 2\n",
    "\n",
    "# GRU\n",
    "latent_dim = 16\n",
    "gru_dim = 16\n",
    "embedding_dim = 8\n",
    "n_layers = 2\n",
    "\n",
    "# cVAE\n",
    "vocab_size = len(vocab_list)\n",
    "gcn_hidden_nodes = n_features + 1\n",
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f8a5c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Start by creating dataset\n",
    "main_path = Path.cwd().parents[0]\n",
    "data_path = main_path / \"data\" / \"RDKit\" / \"rdkit_only_valid_smiles_qm9.pkl\"\n",
    "dataset = GraphData(dataset_path=data_path, max_atoms=max_atoms, node_vec_len=node_vec_len)\n",
    "\n",
    "\n",
    "#### Split data into training and test sets\n",
    "# Get train and test sizes\n",
    "dataset_indices = np.arange(0, len(dataset), 1)\n",
    "train_size = int(np.round(train_size * len(dataset)))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Randomly sample train and test indices\n",
    "train_indices = np.random.choice(dataset_indices, size=train_size, \n",
    "                                                            replace=False)\n",
    "test_indices = np.array(list(set(dataset_indices) - set(train_indices)))\n",
    "\n",
    "# Create dataoaders\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "                          sampler=train_sampler, \n",
    "                          collate_fn=collate_graph_dataset)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "                         sampler=test_sampler,\n",
    "                         collate_fn=collate_graph_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "269543cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = GCN_Encoder(\n",
    "    node_vec_len=node_vec_len,\n",
    "    node_fea_len=n_features,\n",
    "    hidden_fea_len=n_features,\n",
    "    n_conv=n_conv_layers,\n",
    "    n_hidden=n_hidden_layers,\n",
    "    n_outputs=1,\n",
    "    p_dropout=0.1\n",
    ")\n",
    "\n",
    "decoder = GRU_Decoder(\n",
    "    vocab_size=vocab_size,\n",
    "    latent_dim=latent_dim,\n",
    "    property_dim=1,\n",
    "    hidden_size=gru_dim,\n",
    "    n_layers=n_layers,\n",
    "    embedding_dim=embedding_dim\n",
    ").to(device)\n",
    "\n",
    "model = cVAE(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    device=device,\n",
    "    n_gcn_hidden_dim=gcn_hidden_nodes,\n",
    "    n_gru_hidden_dim=gru_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0873bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizer\n",
    "# smiles = [dataset[i][2] for i in range(len(dataset))]\n",
    "\n",
    "# standardizer = Standardizer(torch.Tensor(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c692cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7204004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(model, logits, targets, batch_size, beta=1):\n",
    "    recon_loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    loss_recon = recon_loss_fn(logits, targets)\n",
    "    \n",
    "    kl_loss = -0.5 * torch.sum(1 + model.z_logvar - model.z_mean.pow(2) - model.z_logvar.exp()) / batch_size\n",
    "    loss = loss_recon + beta * kl_loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cdd92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_GPU = False\n",
    "\n",
    "#### Train the model\n",
    "loss = []\n",
    "epoch = []\n",
    "for i in range(n_epochs):\n",
    "    epoch_loss = train_model(\n",
    "        i,\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        loss_function,\n",
    "        use_GPU,\n",
    "        max_atoms,\n",
    "        node_vec_len,\n",
    "    )\n",
    "    loss.append(epoch_loss)\n",
    "    epoch.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c70f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "////<STR>//<STR>/////<STR>\n",
      "4<STR>@C<STR>\\OH<END>(F+H[\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "\n",
    "print(\"\".join([idx2token[i] for i in x[idx].numpy()][1:]))\n",
    "print(\"\".join([idx2token[i] for i in outputs[idx].argmax(1).numpy()][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb889c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 15, 24])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(x, y)  # [batch, seq_len, vocab_size]\n",
    "    \n",
    "print(\"Output shape:\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c2f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token indices:\n",
      " tensor([[ 2,  3,  3,  3,  3,  3, 13,  3,  3, 13,  3,  3,  3,  3, 13],\n",
      "        [ 2,  9,  9,  9,  9, 11, 11,  9,  9, 11,  9,  9,  9,  9, 11],\n",
      "        [ 2,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
      "        [ 2, 12,  3,  6,  6,  3, 18,  6,  3, 18,  3,  3,  3, 18,  3]])\n"
     ]
    }
   ],
   "source": [
    "# pick the most probable token at each step\n",
    "pred_tokens = outputs.argmax(-1)\n",
    "print(\"Predicted token indices:\\n\", pred_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b86836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4#++#H+#H###H#'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([idx2token[i] for i in pred_tokens[3].numpy()][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e88d762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "')\\\\=/12/=<END>][2NO'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([idx2token[i] for i in x[3].numpy()][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9564483e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60fb8eb3",
   "metadata": {},
   "source": [
    "How to handle the data:\n",
    "\n",
    "- Convert data to graph repr\n",
    "- Take the SMILES as a vector\n",
    "- Take the target property as a vector\n",
    "\n",
    "- Combine the target variable with teh graph reprs\n",
    "- Run through GCN\n",
    "- Run throught GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1df72da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GraphData(Dataset):\n",
    "    \"\"\"\n",
    "    Class which creates a custom dataset where each datapoint is a molecule/graph with a node matrix, edge matrix, HOMO-LUMO gap and the smiles representation\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_path: str, node_vec_len: int, max_atoms: int):\n",
    "        # Save attributes\n",
    "        self.node_vec_len = node_vec_len\n",
    "        self.max_atoms = max_atoms\n",
    "\n",
    "        # Open dataset file\n",
    "        df = pd.read_pickle(dataset_path)\n",
    "\n",
    "        # Create lists\n",
    "        self.indices = df.index.to_list()[:1]\n",
    "        self.smiles = df[\"SMILES\"].to_list()[:1]\n",
    "        self.outputs = df[\"gaps\"].to_list()[:1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        # Get smile\n",
    "        smile = self.smiles[i]\n",
    "\n",
    "        # Create MolGraph object using the Graph abstraction\n",
    "        mol = Graph(smile, self.node_vec_len, self.max_atoms)\n",
    "\n",
    "        # Get node and adjacency matrices\n",
    "        node_mat = torch.Tensor(mol.node_mat)\n",
    "        adj_mat = torch.Tensor(mol.adj_mat)\n",
    "\n",
    "        # Get output\n",
    "        output = torch.Tensor([self.outputs[i]])\n",
    "\n",
    "        return (node_mat, adj_mat), output, smile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1280f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fix seeds\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "use_GPU = torch.cuda.is_available()\n",
    "\n",
    "#### Inputs\n",
    "max_atoms = 50\n",
    "node_vec_len = 30\n",
    "train_size = 0.7\n",
    "batch_size = 1\n",
    "hidden_nodes = 30\n",
    "n_conv_layers = 2\n",
    "n_hidden_layers = 2\n",
    "learning_rate = 0.01\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f637b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Start by creating dataset\n",
    "# main_path = Path.cwd().parents[0]\n",
    "data_path = \"data/RDKit/rdkit_only_valid_smiles_qm9.pkl\"\n",
    "dataset = GraphData(dataset_path=data_path, max_atoms=max_atoms, \n",
    "                        node_vec_len=node_vec_len)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6a628086",
   "metadata": {},
   "outputs": [],
   "source": [
    "(node_tensor, adj_tensor), gap, smiles = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e1605e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChemGCN(node_vec_len=node_vec_len, node_fea_len=hidden_nodes,\n",
    "                hidden_fea_len=hidden_nodes, n_conv=n_conv_layers, \n",
    "                n_hidden=n_hidden_layers, n_outputs=1, p_dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f2ffa75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13.7363])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ChemGCN.forward() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m nn_input = (node_mat, adj_mat, output)\n\u001b[32m     28\u001b[39m nn_output = output_std\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m nn_prediction = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mnn_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m nn_prediction.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: ChemGCN.forward() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# Standardizer\n",
    "outputs = [dataset[1] for i in range(len(dataset))]\n",
    "standardizer = Standardizer(torch.Tensor(outputs))\n",
    "\n",
    "# Optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Unpack data\n",
    "node_mat = dataset[0][0]\n",
    "adj_mat = dataset[0][1]\n",
    "output = dataset[1]\n",
    "smiles = dataset[-1]\n",
    "\n",
    "print(output)\n",
    "\n",
    "# Reshape inputs\n",
    "first_dim = int((torch.numel(node_mat)) / (max_atoms * node_vec_len))\n",
    "node_mat = node_mat.reshape(first_dim, max_atoms, node_vec_len)\n",
    "adj_mat = adj_mat.reshape(first_dim, max_atoms, max_atoms)\n",
    "\n",
    "# Standardize output\n",
    "output_std = standardizer.standardize(output)\n",
    "\n",
    "nn_input = (node_mat, adj_mat, output)\n",
    "nn_output = output_std\n",
    "\n",
    "nn_prediction = model(*nn_input)\n",
    "\n",
    "nn_prediction.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
