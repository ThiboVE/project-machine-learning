{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14573d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from library.GCN import ConvolutionLayer, PoolingLayer, GraphData, collate_graph_dataset, Standardizer, Graph\n",
    "from library.cVAE import GCN_Encoder, GRU_Decoder, cVAE\n",
    "from torch.utils.data import DataLoader,SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab3f2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 24\n",
      "Example tokens: ['<PAD>', '<END>', '<STR>', '#', '(', ')', '+', '-', '/', '1', '2', '3', '4', '5', '=', '@', 'C', 'F', 'H', 'N', 'O', '[', '\\\\', ']']\n"
     ]
    }
   ],
   "source": [
    "# Load QM9 SMILES\n",
    "df_qm9 = pd.read_pickle('../data/RDKit/rdkit_only_valid_smiles_qm9.pkl')\n",
    "smiles_list = df_qm9[\"SMILES\"].to_list()\n",
    "\n",
    "# Collect all unique characters\n",
    "charset = set()\n",
    "for smi in smiles_list:\n",
    "    for ch in smi:\n",
    "        charset.add(ch)\n",
    "\n",
    "# Sort for consistency\n",
    "charset = sorted(list(charset))\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = ['<PAD>', '<END>', '<STR>']\n",
    "vocab_list = special_tokens + charset\n",
    "\n",
    "# Create token -> index mapping\n",
    "token2idx = {tok: idx for idx, tok in enumerate(vocab_list)}\n",
    "idx2token = {idx: tok for tok, idx in token2idx.items()}\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab_list))\n",
    "print(\"Example tokens:\", vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d6854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d48a525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    epoch,\n",
    "    model,\n",
    "    training_dataloader,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    use_GPU,\n",
    "    max_atoms,\n",
    "    node_vec_len,\n",
    "):\n",
    "    \"\"\"\n",
    "    Custom function which defines how a model will be trained (per epoch), here the mean-squared loss between prediction and actual value is used as evaluation metric. This function will perform backpropagation which updates the weights of the networks based in this evaluation.\n",
    "    \"\"\"\n",
    "    # Create variables to store losses and error\n",
    "    avg_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    # Switch model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # Go over each batch in the dataloader\n",
    "    for i, dataset in enumerate(training_dataloader):\n",
    "        # Unpack data\n",
    "        node_mat = dataset[0][0]\n",
    "        adj_mat = dataset[0][1]\n",
    "        gap = dataset[1]\n",
    "\n",
    "        smiles = []\n",
    "        for smile in dataset[2]:\n",
    "            char_list = ['<STR>'] + list(smile) + ['<END>']\n",
    "            vocab_idx_list = torch.as_tensor([token2idx[ch] for ch in char_list])\n",
    "            smiles.append(vocab_idx_list)\n",
    "\n",
    "        # ---- Pad sequences ----\n",
    "        # find max length\n",
    "        max_seq_len = max(s.size(0) for s in smiles)\n",
    "\n",
    "        # pad with a PAD token index\n",
    "        PAD_IDX = token2idx['<PAD>']\n",
    "\n",
    "        padded_smiles = torch.full((len(smiles), max_seq_len), PAD_IDX, dtype=torch.long)\n",
    "\n",
    "        for i, seq in enumerate(smiles):\n",
    "            padded_smiles[i, :seq.size(0)] = seq\n",
    "        \n",
    "        batch_size, _ = padded_smiles.size()\n",
    "\n",
    "        # Reshape inputs\n",
    "        node_mat = node_mat.reshape(batch_size, max_atoms, node_vec_len)\n",
    "        adj_mat = adj_mat.reshape(batch_size, max_atoms, max_atoms)\n",
    "\n",
    "        # Package inputs and outputs; check if GPU is enabled\n",
    "        if use_GPU:\n",
    "            model_input = (node_mat.cuda(), adj_mat.cuda(), padded_smiles, gap.cuda())\n",
    "            model_output = padded_smiles\n",
    "        else:\n",
    "            model_input = (node_mat, adj_mat, padded_smiles, gap)\n",
    "            model_output = padded_smiles\n",
    "\n",
    "        # Compute output from network\n",
    "        model_prediction_distribution = model(*model_input) # [batch_size, max_smiles_seq_len, vocab_size]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(model, model_prediction_distribution.permute(0, 2, 1), model_output, batch_size)\n",
    "        avg_loss += loss\n",
    "\n",
    "        # Calculate MAE\n",
    "        # prediction = standardizer.restore(nn_prediction.detach().cpu())\n",
    "        # mae = mean_absolute_error(output, prediction)\n",
    "        # avg_mae += mae\n",
    "\n",
    "        # Set zero gradients for all tensors\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Do backward prop\n",
    "        loss.backward()\n",
    "\n",
    "        # Update optimizer parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Increase count\n",
    "        count += 1\n",
    "\n",
    "    # Calculate avg loss and MAE\n",
    "    avg_loss = avg_loss / count\n",
    "    # avg_mae = avg_mae / count\n",
    "\n",
    "    # Print stats\n",
    "    # print(\n",
    "    #     \"Epoch: [{0}]\\tTraining Loss: [{1:.2f}]\\tTraining MAE: [{2:.2f}]\"\\\n",
    "    #        .format(\n",
    "    #                 epoch, avg_loss, avg_mae\n",
    "    #        )\n",
    "    # )\n",
    "\n",
    "    print(\n",
    "        \"Epoch: [{0}]\\tTraining Loss: [{1:.2f}]\".format(epoch, avg_loss)\n",
    "    )\n",
    "\n",
    "    # Return loss and MAE\n",
    "    return avg_loss #, avg_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0c9b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Inputs\n",
    "n_epochs = 3\n",
    "batch_size = 1000\n",
    "train_size = 0.7\n",
    "learning_rate = 0.01\n",
    "device = \"cpu\"\n",
    "\n",
    "# GCN\n",
    "max_atoms = 30 # fixed value\n",
    "node_vec_len = 16 # fixed value\n",
    "n_features = 32\n",
    "n_conv_layers = 2\n",
    "n_hidden_layers = 2\n",
    "\n",
    "# GRU\n",
    "latent_dim = 16\n",
    "gru_dim = 16\n",
    "embedding_dim = 8\n",
    "n_layers = 2\n",
    "\n",
    "# cVAE\n",
    "vocab_size = len(vocab_list)\n",
    "gcn_hidden_nodes = n_features + 1\n",
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f8a5c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Start by creating dataset\n",
    "main_path = Path.cwd().parents[0]\n",
    "data_path = main_path / \"data\" / \"RDKit\" / \"rdkit_only_valid_smiles_qm9.pkl\"\n",
    "dataset = GraphData(dataset_path=data_path, max_atoms=max_atoms, node_vec_len=node_vec_len)\n",
    "\n",
    "\n",
    "#### Split data into training and test sets\n",
    "# Get train and test sizes\n",
    "dataset_indices = np.arange(0, len(dataset), 1)\n",
    "train_size = int(np.round(train_size * len(dataset)))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Randomly sample train and test indices\n",
    "train_indices = np.random.choice(dataset_indices, size=train_size, \n",
    "                                                            replace=False)\n",
    "test_indices = np.array(list(set(dataset_indices) - set(train_indices)))\n",
    "\n",
    "# Create dataoaders\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "                          sampler=train_sampler, \n",
    "                          collate_fn=collate_graph_dataset)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "                         sampler=test_sampler,\n",
    "                         collate_fn=collate_graph_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "269543cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = GCN_Encoder(\n",
    "    node_vec_len=node_vec_len,\n",
    "    node_fea_len=n_features,\n",
    "    hidden_fea_len=n_features,\n",
    "    n_conv=n_conv_layers,\n",
    "    n_hidden=n_hidden_layers,\n",
    "    n_outputs=1,\n",
    "    p_dropout=0.1\n",
    ")\n",
    "\n",
    "decoder = GRU_Decoder(\n",
    "    vocab_size=vocab_size,\n",
    "    latent_dim=latent_dim,\n",
    "    property_dim=1,\n",
    "    hidden_size=gru_dim,\n",
    "    n_layers=n_layers,\n",
    "    embedding_dim=embedding_dim\n",
    ").to(device)\n",
    "\n",
    "model = cVAE(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    device=device,\n",
    "    n_gcn_hidden_dim=gcn_hidden_nodes,\n",
    "    n_gru_hidden_dim=gru_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0873bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizer\n",
    "# smiles = [dataset[i][2] for i in range(len(dataset))]\n",
    "\n",
    "# standardizer = Standardizer(torch.Tensor(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c692cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7204004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(model, logits, targets, batch_size, beta=1):\n",
    "    recon_loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    loss_recon = recon_loss_fn(logits, targets)\n",
    "    \n",
    "    kl_loss = -0.5 * torch.sum(1 + model.z_logvar - model.z_mean.pow(2) - model.z_logvar.exp()) / batch_size\n",
    "    loss = loss_recon + beta * kl_loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7cdd92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]\tTraining Loss: [2.29]\n",
      "Epoch: [1]\tTraining Loss: [1.47]\n",
      "Epoch: [2]\tTraining Loss: [1.32]\n"
     ]
    }
   ],
   "source": [
    "use_GPU = False\n",
    "\n",
    "#### Train the model\n",
    "loss = []\n",
    "epoch = []\n",
    "for i in range(n_epochs):\n",
    "    epoch_loss = train_model(\n",
    "        i,\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        loss_function,\n",
    "        use_GPU,\n",
    "        max_atoms,\n",
    "        node_vec_len,\n",
    "    )\n",
    "    loss.append(epoch_loss)\n",
    "    epoch.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99110a29",
   "metadata": {},
   "source": [
    "Need to add:\n",
    "- a form of accuracy\n",
    "- CV for hyperparameter tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
