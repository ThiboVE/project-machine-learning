{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14573d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "from library.GCN import ConvolutionLayer, PoolingLayer, GraphData, collate_graph_dataset, Standardizer, Graph\n",
    "from library.cVAE import GCN_Encoder, GRU_Decoder, cVAE\n",
    "from torch.utils.data import DataLoader,SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "308469ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(smiles_list):\n",
    "    # Collect all unique characters\n",
    "    charset = set()\n",
    "    for smi in smiles_list:\n",
    "        for ch in smi:\n",
    "            charset.add(ch)\n",
    "\n",
    "    # Sort for consistency\n",
    "    charset = sorted(list(charset))\n",
    "\n",
    "    # Add special tokens\n",
    "    special_tokens = ['<PAD>', '<END>', '<STR>']\n",
    "    vocab_list = special_tokens + charset\n",
    "\n",
    "    return vocab_list, len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f92d711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 24\n",
      "Example tokens: ['<PAD>', '<END>', '<STR>', '#', '(', ')', '+', '-', '/', '1', '2', '3', '4', '5', '=', '@', 'C', 'F', 'H', 'N', 'O', '[', '\\\\', ']']\n"
     ]
    }
   ],
   "source": [
    "# Load QM9 SMILES\n",
    "df_qm9 = pd.read_pickle('../data/RDKit/rdkit_only_valid_smiles_qm9.pkl')\n",
    "smiles_list = df_qm9[\"SMILES\"].to_list()\n",
    "\n",
    "vocab_list, vocab_size = get_vocab(smiles_list)\n",
    "\n",
    "# Create token2index mapping and its inverse\n",
    "token2idx = {tok: idx for idx, tok in enumerate(vocab_list)}\n",
    "idx2token = {idx: tok for tok, idx in token2idx.items()}\n",
    "\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Example tokens:\", vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d6854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(smiles):\n",
    "    # find max length\n",
    "    max_seq_len = max(s.size(0) for s in smiles)\n",
    "\n",
    "    # pad with a PAD token index\n",
    "    pad_idx = token2idx['<PAD>']\n",
    "\n",
    "    padded_smiles = torch.full((len(smiles), max_seq_len), pad_idx, dtype=torch.long)\n",
    "\n",
    "    for i, seq in enumerate(smiles):\n",
    "        padded_smiles[i, :seq.size(0)] = seq\n",
    "    \n",
    "    return padded_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ff830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_idxs(smiles):\n",
    "    modified_smiles = [] # [batch_size, seq_len]\n",
    "    for smile in smiles:\n",
    "        # Add the start and end tokens to the smiles\n",
    "        char_list = ['<STR>'] + list(smile) + ['<END>']\n",
    "        # Convert tokens to indices\n",
    "        vocab_idx_list = torch.as_tensor([token2idx[ch] for ch in char_list])\n",
    "        modified_smiles.append(vocab_idx_list)\n",
    "    \n",
    "    padded_smiles = padding(modified_smiles)\n",
    "    return padded_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "742c52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_level_accuracy(logits, targets, pad_idx=None):\n",
    "    \"\"\"\n",
    "    logits:  (batch, seq_len, vocab_size)\n",
    "    targets: (batch, seq_len)\n",
    "    pad_idx: integer for PAD token (optional)\n",
    "    \"\"\"\n",
    "    # Predicted token IDs\n",
    "    preds = torch.argmax(logits, dim=-1)  # (batch, seq_len)\n",
    "\n",
    "    # Boolean matrix: correct prediction per position\n",
    "    correct = (preds == targets)  # (batch, seq_len)\n",
    "\n",
    "    # Mask out padding positions\n",
    "    if pad_idx is not None:\n",
    "        mask = (targets != pad_idx).float()\n",
    "        correct = correct.float() * mask\n",
    "        lengths = mask.sum(dim=1)               # seq_len per SMILES (no pads)\n",
    "    else:\n",
    "        correct = correct.float()\n",
    "        lengths = torch.full(\n",
    "            (targets.size(0),), targets.size(1), device=targets.device\n",
    "        )\n",
    "\n",
    "    # Per-SMILES reconstruction accuracy\n",
    "    per_smiles_acc = correct.sum(dim=1) / lengths   # (batch,)\n",
    "\n",
    "    # Average over all SMILES in the batch\n",
    "    avg_acc = per_smiles_acc.mean()\n",
    "\n",
    "    return avg_acc.item(), per_smiles_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d48a525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(\n",
    "    epoch,\n",
    "    model,\n",
    "    training_dataloader,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    use_GPU,\n",
    "    max_atoms,\n",
    "    node_vec_len,\n",
    "):\n",
    "    \"\"\"\n",
    "    Custom function which defines how a model will be trained (per epoch), here the mean-squared loss between prediction and actual value is used as evaluation metric. This function will perform backpropagation which updates the weights of the networks based in this evaluation.\n",
    "    \"\"\"\n",
    "    # Create variables to store losses and error\n",
    "    avg_loss = 0\n",
    "    avg_validity = 0\n",
    "    count = 0\n",
    "    epoch_acc_sum = 0.0\n",
    "    epoch_smiles_count = 0\n",
    "    # Switch model to train mode\n",
    "    model.train()\n",
    "    # Go over each batch in the dataloader\n",
    "    for i, dataset in enumerate(training_dataloader):\n",
    "        # Unpack data\n",
    "        node_mat = dataset[0][0]\n",
    "        adj_mat = dataset[0][1]\n",
    "        gap = dataset[1]\n",
    "        \n",
    "        padded_smiles = smiles_to_idxs(dataset[2])\n",
    "        batch_size, max_seq_len = padded_smiles.size()\n",
    "\n",
    "        # Reshape inputs\n",
    "        node_mat = node_mat.reshape(batch_size, max_atoms, node_vec_len)\n",
    "        adj_mat = adj_mat.reshape(batch_size, max_atoms, max_atoms)\n",
    "\n",
    "        # Package inputs and outputs; check if GPU is enabled\n",
    "        if use_GPU:\n",
    "            model_input = (node_mat.cuda(), adj_mat.cuda(), padded_smiles, gap.cuda())\n",
    "            model_output = padded_smiles\n",
    "        else:\n",
    "            model_input = (node_mat, adj_mat, padded_smiles, gap)\n",
    "            model_output = padded_smiles\n",
    "\n",
    "        # Compute output from network\n",
    "        model_prediction_distribution = model(*model_input) # [batch_size, max_smiles_seq_len, vocab_size]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(model, model_prediction_distribution.permute(0, 2, 1), model_output, batch_size)\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        # returns (batch_avg, per_smiles_vector)\n",
    "        batch_avg, per_smiles_acc = smiles_level_accuracy(model_prediction_distribution, padded_smiles)\n",
    "        epoch_acc_sum += per_smiles_acc.sum().item()   # sum of accuracies for this batch\n",
    "        epoch_smiles_count += per_smiles_acc.size(0)   # number of molecules in batch\n",
    "\n",
    "        # max_prob_smiles_tokens = torch.argmax(model_prediction_distribution, dim=-1)\n",
    "        # max_prob_smiles = \"\".join(idx2token[idx] for idx in max_prob_smiles_tokens[0].numpy() if idx not in (0, 1, 2))\n",
    "        \n",
    "        # Check if the constructed smiles is a valid molecule\n",
    "        # if Chem.MolFromSmiles(max_prob_smiles) is not None:\n",
    "        #     avg_validity += 1\n",
    "\n",
    "        # Set zero gradients for all tensors\n",
    "        optimizer.zero_grad()\n",
    "        # Do backward prop\n",
    "        loss.backward()\n",
    "        # Update optimizer parameters\n",
    "        optimizer.step()\n",
    "        # Increase count\n",
    "        count += 1\n",
    "    # Calculate avg loss and validity\n",
    "    avg_loss = avg_loss / count\n",
    "    # avg_validity = avg_validity / count\n",
    "    epoch_smiles_accuracy = epoch_acc_sum / epoch_smiles_count\n",
    "    # Print stats\n",
    "    print(f\"Epoch: [{epoch}]\\tTraining Loss: [{avg_loss:.2f}]\\tReconstruction accuracy: [{epoch_smiles_accuracy}]\")\n",
    "    # Return loss and validity\n",
    "    return avg_loss, epoch_smiles_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b01e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(\n",
    "    model,\n",
    "    test_dataloader,\n",
    "    loss_fn,\n",
    "    use_GPU,\n",
    "    max_atoms,\n",
    "    node_vec_len,\n",
    "):\n",
    "    \"\"\"\n",
    "    Test the ChemGCN model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : ChemGCN\n",
    "        ChemGCN model object\n",
    "    test_dataloader : data.DataLoader\n",
    "        Test DataLoader\n",
    "    loss_fn : like nn.MSELoss()\n",
    "        Model loss function\n",
    "    standardizer : Standardizer\n",
    "        Standardizer object\n",
    "    use_GPU: bool\n",
    "        Whether to use GPU\n",
    "    max_atoms: int\n",
    "        Maximum number of atoms in graph\n",
    "    node_vec_len: int\n",
    "        Maximum node vector length in graph\n",
    "    Returns\n",
    "    -------\n",
    "    test_loss : float\n",
    "        Test loss\n",
    "    test_mae : float\n",
    "        Test MAE\n",
    "    \"\"\"\n",
    "    # Create variables to store losses and error\n",
    "    test_loss = 0\n",
    "    epoch_acc_sum = 0.0\n",
    "    epoch_smiles_count = 0\n",
    "    count = 0\n",
    "    # Switch model to train mode\n",
    "    model.eval()\n",
    "    # Go over each batch in the dataloader\n",
    "    for i, dataset in enumerate(test_dataloader):\n",
    "        # Unpack data\n",
    "        node_mat = dataset[0][0]\n",
    "        adj_mat = dataset[0][1]\n",
    "        gap = dataset[1]\n",
    "        padded_smiles = smiles_to_idxs(dataset[2])\n",
    "        batch_size, _ = padded_smiles.size()\n",
    "        # Reshape inputs\n",
    "        node_mat = node_mat.reshape(batch_size, max_atoms, node_vec_len)\n",
    "        adj_mat = adj_mat.reshape(batch_size, max_atoms, max_atoms)\n",
    "        # Package inputs and outputs; check if GPU is enabled\n",
    "        if use_GPU:\n",
    "            model_input = (node_mat.cuda(), adj_mat.cuda(), padded_smiles, gap.cuda())\n",
    "            model_output = padded_smiles\n",
    "        else:\n",
    "            model_input = (node_mat, adj_mat, padded_smiles, gap)\n",
    "            model_output = padded_smiles\n",
    "        # Compute output from network\n",
    "        model_prediction_distribution = model(*model_input) # [batch_size, max_smiles_seq_len, vocab_size]\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(model, model_prediction_distribution.permute(0, 2, 1), model_output, batch_size)\n",
    "        test_loss += loss.item()\n",
    "        # returns (batch_avg, per_smiles_vector)\n",
    "        batch_avg, per_smiles_acc = smiles_level_accuracy(model_prediction_distribution, padded_smiles)\n",
    "        epoch_acc_sum += per_smiles_acc.sum().item()   # sum of accuracies for this batch\n",
    "        epoch_smiles_count += per_smiles_acc.size(0)   # number of molecules in batch\n",
    "        \n",
    "        # Increase count\n",
    "        count += 1\n",
    "    # Calculate avg loss and MAE\n",
    "    test_loss = test_loss / count\n",
    "    epoch_smiles_accuracy = epoch_acc_sum / epoch_smiles_count\n",
    "    # Print stats\n",
    "    print(f\"Training Loss: [{test_loss:.2f}]\\tReconstruction accuracy: [{epoch_smiles_accuracy}]\")\n",
    "    # Return loss and validity\n",
    "    return test_loss, epoch_smiles_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Inputs\n",
    "n_epochs = 4\n",
    "batch_size = 1000\n",
    "train_size = 0.7\n",
    "learning_rate = 0.01\n",
    "device = \"cpu\"\n",
    "\n",
    "# GCN\n",
    "max_atoms = 30 # fixed value\n",
    "node_vec_len = 16 # fixed value\n",
    "n_hidden = 32\n",
    "n_conv_layers = 2\n",
    "n_hidden_layers = 2\n",
    "\n",
    "# GRU\n",
    "latent_dim = 16\n",
    "gru_dim = 16\n",
    "embedding_dim = 8\n",
    "n_gru_layers = 2 # stacked GRUs\n",
    "n_fc_layers = 3 # 3 dense layers after GRU\n",
    "\n",
    "# cVAE\n",
    "gcn_hidden_nodes = n_hidden + 1\n",
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f8a5c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Start by creating dataset\n",
    "main_path = Path.cwd().parents[0]\n",
    "data_path = main_path / \"data\" / \"RDKit\" / \"rdkit_only_valid_smiles_qm9.pkl\"\n",
    "dataset = GraphData(dataset_path=data_path, max_atoms=max_atoms, node_vec_len=node_vec_len)\n",
    "\n",
    "\n",
    "#### Split data into training and test sets\n",
    "# Get train and test sizes\n",
    "dataset_indices = np.arange(0, len(dataset), 1)\n",
    "train_size = int(np.round(train_size * len(dataset)))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Randomly sample train and test indices\n",
    "train_indices = np.random.choice(dataset_indices, size=train_size, \n",
    "                                                            replace=False)\n",
    "test_indices = np.array(list(set(dataset_indices) - set(train_indices)))\n",
    "\n",
    "# Create dataoaders\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "                          sampler=train_sampler, \n",
    "                          collate_fn=collate_graph_dataset)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "                         sampler=test_sampler,\n",
    "                         collate_fn=collate_graph_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269543cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = GCN_Encoder(\n",
    "    node_vec_len=node_vec_len,\n",
    "    node_fea_len=n_hidden,\n",
    "    hidden_fea_len=n_hidden,\n",
    "    n_conv=n_conv_layers,\n",
    "    n_hidden=n_hidden_layers,\n",
    "    n_outputs=1,\n",
    "    p_dropout=0.1\n",
    ")\n",
    "\n",
    "decoder = GRU_Decoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_size=gru_dim,\n",
    "    n_gru_layers=n_gru_layers,\n",
    "    n_fc_layers=n_fc_layers\n",
    ").to(device)\n",
    "\n",
    "model = cVAE(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    device=device,\n",
    "    n_gcn_hidden_dim=gcn_hidden_nodes,\n",
    "    n_gru_hidden_dim=gru_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0873bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizer\n",
    "# smiles = [dataset[i][2] for i in range(len(dataset))]\n",
    "\n",
    "# standardizer = Standardizer(torch.Tensor(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c692cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7204004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(model, logits, targets, batch_size, beta=1):\n",
    "    recon_loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    loss_recon = recon_loss_fn(logits, targets)\n",
    "    \n",
    "    kl_loss = -0.5 * torch.sum(1 + model.z_logvar - model.z_mean.pow(2) - model.z_logvar.exp()) / batch_size\n",
    "    loss = loss_recon + beta * kl_loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7cdd92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]\tTraining Loss: [2.69]\tReconstruction accuracy: [0.11064978921973642]\n",
      "Epoch: [1]\tTraining Loss: [1.94]\tReconstruction accuracy: [0.23356067077755668]\n",
      "Epoch: [2]\tTraining Loss: [1.52]\tReconstruction accuracy: [0.35046428435929206]\n",
      "Epoch: [3]\tTraining Loss: [1.33]\tReconstruction accuracy: [0.38661921760899376]\n"
     ]
    }
   ],
   "source": [
    "use_GPU = False\n",
    "\n",
    "#### Train the model\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "epoch = []\n",
    "for i in range(n_epochs):\n",
    "    epoch_loss, epoch_accuarcy = train_model(\n",
    "        i,\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        loss_function,\n",
    "        use_GPU,\n",
    "        max_atoms,\n",
    "        node_vec_len,\n",
    "    )\n",
    "    train_loss.append(epoch_loss)\n",
    "    epoch.append(i)\n",
    "    train_accuracy.append(epoch_accuarcy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "250cee09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: [1.21]\tReconstruction accuracy: [0.41005098154447256]\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = test_model(model, test_loader, loss_function, use_GPU, max_atoms, node_vec_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d41bfb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.33\n",
      "Training accuracy: 0.39\n",
      "Test Loss: 1.21\n",
      "Test accuracy: 0.41\n"
     ]
    }
   ],
   "source": [
    "#### Print final results\n",
    "print(f\"Training Loss: {train_loss[-1]:.2f}\")\n",
    "print(f\"Training accuracy: {train_accuracy[-1]:.2f}\")\n",
    "print(f\"Test Loss: {test_loss:.2f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99110a29",
   "metadata": {},
   "source": [
    "Need to add:\n",
    "- a form of accuracy\n",
    "- CV for hyperparameter tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
