{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6da8359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from library.functions_to_abstract_data import extract_qm9_data\n",
    "from torch_geometric.datasets import QM9\n",
    "from rdkit import Chem\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "from rdkit.Chem import Descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cdf3de",
   "metadata": {},
   "source": [
    "Import the dataframe that has only valid smiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e0aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_smiles = pd.read_pickle(f'../data/RDKit/rdkit_only_valid_smiles_qm9.pkl')\n",
    "df_valid = df_valid_smiles.drop(columns=df_valid_smiles.columns[-2:]) # Take all colmumns beside the last two (smiles and gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbfe474",
   "metadata": {},
   "source": [
    "Remove redundant features (>0.90 correlation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e577472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 40 features; remaining: 177\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df_valid.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than 0.90\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "\n",
    "# Drop them\n",
    "df_reduced = df_valid.drop(columns=to_drop)\n",
    "\n",
    "print(f\"Dropped {len(to_drop)} features; remaining: {df_reduced.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a75ab",
   "metadata": {},
   "source": [
    "Apply PCA (creates linear combinations of the features and ensures these linear combinations are orthogonal/independant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ff4969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 177\n",
      "Reduced to 78 principal components\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_reduced)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "print(f\"Original features: {df_reduced.shape[1]}\")\n",
    "print(f\"Reduced to {pca_data.shape[1]} principal components\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
