{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f01cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from library.GCN import *\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader,SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11badb3b",
   "metadata": {},
   "source": [
    "### Chemical accuracy is 0.043 eV, so MAE should be lower than this for the model to be chemically accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5a8e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Fix seeds\n",
    "# np.random.seed(10)\n",
    "# torch.manual_seed(10)\n",
    "# use_GPU = torch.cuda.is_available()\n",
    "\n",
    "# #### Inputs\n",
    "# max_atoms = 30 # fixed value\n",
    "# node_vec_len = 16 # fixed value\n",
    "# train_size = 0.7\n",
    "# batch_size = 1000\n",
    "# hidden_nodes = 32\n",
    "# n_conv_layers = 2\n",
    "# n_hidden_layers = 2\n",
    "# learning_rate = 0.01\n",
    "# n_epochs = 15\n",
    "\n",
    "# #### Start by creating dataset\n",
    "# main_path = Path.cwd().parents[0]\n",
    "# data_path = main_path / \"data\" / \"RDKit\" / \"rdkit_only_valid_smiles_qm9.pkl\"\n",
    "# dataset = GraphData(dataset_path=data_path, max_atoms=max_atoms, \n",
    "#                         node_vec_len=node_vec_len)\n",
    "\n",
    "# #### Split data into training and test sets\n",
    "# # Get train and test sizes\n",
    "# dataset_indices = np.arange(0, len(dataset), 1)\n",
    "# train_size = int(np.round(train_size * len(dataset)))\n",
    "# test_size = len(dataset) - train_size\n",
    "\n",
    "# # Randomly sample train and test indices\n",
    "# train_indices = np.random.choice(dataset_indices, size=train_size, \n",
    "#                                                             replace=False)\n",
    "# test_indices = np.array(list(set(dataset_indices) - set(train_indices)))\n",
    "\n",
    "# # Create dataoaders\n",
    "# train_sampler = SubsetRandomSampler(train_indices)\n",
    "# test_sampler = SubsetRandomSampler(test_indices)\n",
    "# train_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "#                           sampler=train_sampler, \n",
    "#                           collate_fn=collate_graph_dataset)\n",
    "# test_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "#                          sampler=test_sampler,\n",
    "#                          collate_fn=collate_graph_dataset)\n",
    "\n",
    "# #### Initialize model, standardizer, optimizer, and loss function\n",
    "# # Model\n",
    "# model = ChemGCN(node_vec_len=node_vec_len, node_fea_len=hidden_nodes,\n",
    "#                 hidden_fea_len=hidden_nodes, n_conv=n_conv_layers, \n",
    "#                 n_hidden=n_hidden_layers, n_outputs=1, p_dropout=0.1)\n",
    "# # Transfer to GPU if needed\n",
    "# if use_GPU:\n",
    "#     model.cuda()\n",
    "\n",
    "# # Standardizer\n",
    "# outputs = [dataset[i][1] for i in range(len(dataset))]\n",
    "# standardizer = Standardizer(torch.Tensor(outputs))\n",
    "\n",
    "# # Optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Loss function\n",
    "# # loss_fn = torch.nn.MSELoss()\n",
    "# loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "# #### Train the model\n",
    "# loss = []\n",
    "# mae = []\n",
    "# epoch = []\n",
    "# for i in range(n_epochs):\n",
    "#     epoch_loss, epoch_mae = train_model(\n",
    "#         i,\n",
    "#         model,\n",
    "#         train_loader,\n",
    "#         optimizer,\n",
    "#         loss_fn,\n",
    "#         standardizer,\n",
    "#         use_GPU,\n",
    "#         max_atoms,\n",
    "#         node_vec_len,\n",
    "#     )\n",
    "#     loss.append(epoch_loss)\n",
    "#     mae.append(epoch_mae)\n",
    "#     epoch.append(i)\n",
    "\n",
    "# #### Test the model\n",
    "# # Call test model function\n",
    "# test_loss, test_mae = test_model(model, test_loader, loss_fn, standardizer,\n",
    "#                                  use_GPU, max_atoms, node_vec_len)\n",
    "\n",
    "# #### Print final results\n",
    "# print(f\"Training Loss: {loss[-1]:.2f}\")\n",
    "# print(f\"Training MAE: {mae[-1]:.2f}\")\n",
    "# print(f\"Test Loss: {test_loss:.2f}\")\n",
    "# print(f\"Test MAE: {test_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede45fa4",
   "metadata": {},
   "source": [
    "Accuracy metrics: MAE, (R)MSE, $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ec6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fix seeds\n",
    "np.random.seed(10)\n",
    "torch.manual_seed(10)\n",
    "use_GPU = torch.cuda.is_available()\n",
    "\n",
    "#### Inputs\n",
    "max_atoms = 30 # fixed value\n",
    "node_vec_len = 16 # fixed value\n",
    "# batch_size = 1000\n",
    "# hidden_nodes = 32\n",
    "# n_conv_layers = 2\n",
    "# n_hidden_layers = 2\n",
    "# learning_rate = 0.01\n",
    "n_epochs = 30\n",
    "\n",
    "main_path = Path.cwd().parents[0]\n",
    "data_path = main_path / \"data\" / \"RDKit\" / \"rdkit_only_valid_smiles_qm9.pkl\"\n",
    "dataset = GraphData(dataset_path=data_path, max_atoms=max_atoms, \n",
    "                        node_vec_len=node_vec_len)\n",
    "dataset_indices = np.arange(0, len(dataset), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f996929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_param_combinations(param_grid):\n",
    "    keys = param_grid.keys()\n",
    "    values = param_grid.values()\n",
    "    for combo in itertools.product(*values):\n",
    "        yield dict(zip(keys, combo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e683b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_results = []     # Test MAE per outer fold\n",
    "best_hyperparams = []  # Best params per fold\n",
    "\n",
    "param_grid = {\n",
    "    \"batch_size\": [256, 512, 1024],\n",
    "    \"hidden_nodes\": [16, 32, 64],\n",
    "    \"n_conv_layers\": [1, 2, 3],\n",
    "    \"n_hidden_layers\": [1, 2, 3],\n",
    "    \"learning_rate\": [0.001, 0.005, 0.01]\n",
    "}\n",
    "\n",
    "inner_cv = KFold(n_splits=10, shuffle=True, random_state=10)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "dataset_indices = np.arange(len(dataset))\n",
    "\n",
    "for outer_fold, (train_val_idx, test_idx) in enumerate(outer_cv.split(dataset_indices)):\n",
    "    print(f\"\\n===== OUTER FOLD {outer_fold+1}/5 =====\")\n",
    "\n",
    "    # Outer test loader\n",
    "    test_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=max(param_grid[\"batch_size\"]),  # largest batch\n",
    "        sampler=SubsetRandomSampler(test_idx),\n",
    "        collate_fn=collate_graph_dataset\n",
    "    )\n",
    "\n",
    "    # Store validation mean MAE for each configuration\n",
    "    performance_dict = {}\n",
    "\n",
    "    # ================================\n",
    "    # INNER GRID SEARCH\n",
    "    # ================================\n",
    "    for params in get_param_combinations(param_grid):\n",
    "        print(f\"\\nTesting hyperparameters: {params}\")\n",
    "        inner_fold_mae = []\n",
    "\n",
    "        for inner_fold, (inner_train_idx, inner_val_idx) in enumerate(\n",
    "            inner_cv.split(train_val_idx)\n",
    "        ):\n",
    "            print(f\"  Inner Fold {inner_fold + 1}/10\")\n",
    "\n",
    "            train_idx = train_val_idx[inner_train_idx]\n",
    "            val_idx = train_val_idx[inner_val_idx]\n",
    "\n",
    "            # Build loaders\n",
    "            train_loader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=params[\"batch_size\"],\n",
    "                sampler=SubsetRandomSampler(train_idx),\n",
    "                collate_fn=collate_graph_dataset,\n",
    "            )\n",
    "            \n",
    "            val_loader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=params[\"batch_size\"],\n",
    "                sampler=SubsetRandomSampler(val_idx),\n",
    "                collate_fn=collate_graph_dataset,\n",
    "            )\n",
    "\n",
    "            # Build new model\n",
    "            model = ChemGCN(\n",
    "                node_vec_len=node_vec_len,\n",
    "                node_fea_len=params[\"hidden_nodes\"],\n",
    "                hidden_fea_len=params[\"hidden_nodes\"],\n",
    "                n_conv=params[\"n_conv_layers\"],\n",
    "                n_hidden=params[\"n_hidden_layers\"],\n",
    "                n_outputs=1,\n",
    "                p_dropout=0.1,\n",
    "            )\n",
    "            if use_GPU:\n",
    "                model.cuda()\n",
    "\n",
    "            # Standardizer from training fold only\n",
    "            outputs = [dataset[i][1] for i in train_idx]\n",
    "            standardizer = Standardizer(torch.Tensor(outputs))\n",
    "\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "            loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "            # -------- Training Loop for Inner CV --------\n",
    "            for epoch in range(n_epochs):\n",
    "                train_model(\n",
    "                    epoch, model, train_loader, optimizer, loss_fn,\n",
    "                    standardizer, use_GPU, max_atoms, node_vec_len\n",
    "                )\n",
    "\n",
    "            # -------- Validation evaluation --------\n",
    "            _, val_mae = test_model(\n",
    "                model, val_loader, loss_fn, standardizer,\n",
    "                use_GPU, max_atoms, node_vec_len\n",
    "            )\n",
    "            inner_fold_mae.append(val_mae)\n",
    "\n",
    "        # Save mean validation performance\n",
    "        performance_dict[tuple(params.items())] = np.mean(inner_fold_mae)\n",
    "        print(f\"  Mean Validation MAE = {performance_dict[tuple(params.items())]:.4f}\")\n",
    "\n",
    "    # ================================\n",
    "    # SELECT BEST INNER-CV PARAMETERS\n",
    "    # ================================\n",
    "    best_params = min(performance_dict, key=performance_dict.get)\n",
    "    best_params = dict(best_params)\n",
    "    best_hyperparams.append(best_params)\n",
    "\n",
    "    print(f\"\\n>>> Best inner-CV params for Fold {outer_fold+1}: {best_params}\")\n",
    "\n",
    "    # ================================\n",
    "    # RETRAIN ON FULL TRAIN+VALIDATION\n",
    "    # ================================\n",
    "    full_train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=best_params[\"batch_size\"],\n",
    "        sampler=SubsetRandomSampler(train_val_idx),\n",
    "        collate_fn=collate_graph_dataset,\n",
    "    )\n",
    "\n",
    "    model = ChemGCN(\n",
    "        node_vec_len=node_vec_len,\n",
    "        node_fea_len=best_params[\"hidden_nodes\"],\n",
    "        hidden_fea_len=best_params[\"hidden_nodes\"],\n",
    "        n_conv=best_params[\"n_conv_layers\"],\n",
    "        n_hidden=best_params[\"n_hidden_layers\"],\n",
    "        n_outputs=1,\n",
    "        p_dropout=0.1,\n",
    "    )\n",
    "    if use_GPU:\n",
    "        model.cuda()\n",
    "\n",
    "    outputs = [dataset[i][1] for i in train_val_idx]\n",
    "    standardizer = Standardizer(torch.Tensor(outputs))\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_model(\n",
    "            epoch, model, full_train_loader, optimizer, loss_fn,\n",
    "            standardizer, use_GPU, max_atoms, node_vec_len\n",
    "        )\n",
    "\n",
    "    # ================================\n",
    "    # FINAL TEST ON OUTER FOLD\n",
    "    # ================================\n",
    "    test_loss, test_mae = test_model(\n",
    "        model, test_loader, loss_fn, standardizer,\n",
    "        use_GPU, max_atoms, node_vec_len\n",
    "    )\n",
    "\n",
    "    outer_results.append(test_mae)\n",
    "    print(f\"===== Outer Fold {outer_fold+1} Test MAE: {test_mae:.4f} =====\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
